{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1cf8bd-ad6d-48f0-a504-ede1ff9ebf85",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee2a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429226eb-2a93-486c-98a8-6c875aaa3447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Détection automatique du périphérique (GPU si disponible, sinon CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d94de9-0bfe-41cf-aac0-7828d05e41c2",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8994a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_file=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = sorted(os.listdir(img_dir))\n",
    "        \n",
    "        if labels_file:\n",
    "            self.labels = pd.read_csv(labels_file, header=None, names=['label'])\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels.iloc[idx, 0]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "        \n",
    "def get_data_loaders(train_img_dir, train_labels_file, val_img_dir, batch_size=256):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ImageDataset(train_img_dir, train_labels_file, transform=transform)\n",
    "    val_dataset = ImageDataset(val_img_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f6916-8c2b-4305-9e75-931fdf3cda95",
   "metadata": {},
   "source": [
    "# Training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eeb9751-4bfa-49fc-8eea-7e0aba65fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \n",
    "    # Load the pretrained ResNet18 model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Replace the final classification layer with a binary output layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Binary classification loss function \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Adam optimizer for weight updates\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # The training mode\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to the selected device\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Display progress every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:\n",
    "                print(f\"  Batch {batch_idx+1}/{total_batches} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), './mymodel.pth')\n",
    "    print(\" Model saved to './mymodel.pth'\")\n",
    "\n",
    "def predict(val_loader):\n",
    "\n",
    "    # Load the same model architecture and weights\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    model.load_state_dict(torch.load('./mymodel.pth'))\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    predictions = []\n",
    "    print(\"Generating predictions...\")\n",
    "\n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, inputs in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "                print(f\"  Processed batch {batch_idx+1}/{len(val_loader)}\")\n",
    "\n",
    "    # Write binary predictions to a text file\n",
    "    with open('./label_val.txt', 'w') as f:\n",
    "        for pred in predictions:\n",
    "            f.write(f\"{int(pred > 0.5)}\\n\")\n",
    "\n",
    "    print(\" Predictions saved to './label_val.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84617f-715d-42e3-9fa3-09d8f041e9df",
   "metadata": {},
   "source": [
    "# Main training function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eee7511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/missa1/venvs/torch-gpu-py311/lib64/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/missa1/venvs/torch-gpu-py311/lib64/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/missa1/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "  Batch 10/391 - Loss: 0.2385\n",
      "  Batch 20/391 - Loss: 0.2218\n",
      "  Batch 30/391 - Loss: 0.2027\n",
      "  Batch 40/391 - Loss: 0.1749\n",
      "  Batch 50/391 - Loss: 0.1716\n",
      "  Batch 60/391 - Loss: 0.1740\n",
      "  Batch 70/391 - Loss: 0.1644\n",
      "  Batch 80/391 - Loss: 0.1654\n",
      "  Batch 90/391 - Loss: 0.1519\n",
      "  Batch 100/391 - Loss: 0.1374\n",
      "  Batch 110/391 - Loss: 0.1704\n",
      "  Batch 120/391 - Loss: 0.1251\n",
      "  Batch 130/391 - Loss: 0.1722\n",
      "  Batch 140/391 - Loss: 0.1904\n",
      "  Batch 150/391 - Loss: 0.1468\n",
      "  Batch 160/391 - Loss: 0.1289\n",
      "  Batch 170/391 - Loss: 0.1561\n",
      "  Batch 180/391 - Loss: 0.1287\n",
      "  Batch 190/391 - Loss: 0.1186\n",
      "  Batch 200/391 - Loss: 0.1452\n",
      "  Batch 210/391 - Loss: 0.1292\n",
      "  Batch 220/391 - Loss: 0.1075\n",
      "  Batch 230/391 - Loss: 0.1669\n",
      "  Batch 240/391 - Loss: 0.1938\n",
      "  Batch 250/391 - Loss: 0.1732\n",
      "  Batch 260/391 - Loss: 0.1460\n",
      "  Batch 270/391 - Loss: 0.0988\n",
      "  Batch 280/391 - Loss: 0.1466\n",
      "  Batch 290/391 - Loss: 0.1836\n",
      "  Batch 300/391 - Loss: 0.1391\n",
      "  Batch 310/391 - Loss: 0.1217\n",
      "  Batch 320/391 - Loss: 0.1844\n",
      "  Batch 330/391 - Loss: 0.1031\n",
      "  Batch 340/391 - Loss: 0.1527\n",
      "  Batch 350/391 - Loss: 0.1538\n",
      "  Batch 360/391 - Loss: 0.1515\n",
      "  Batch 370/391 - Loss: 0.1284\n",
      "  Batch 380/391 - Loss: 0.1378\n",
      "  Batch 390/391 - Loss: 0.1437\n",
      "  Batch 391/391 - Loss: 0.1391\n",
      "Epoch 1 completed. Average Loss: 0.1601\n",
      "\n",
      "Epoch 2/10\n",
      "  Batch 10/391 - Loss: 0.1432\n",
      "  Batch 20/391 - Loss: 0.1466\n",
      "  Batch 30/391 - Loss: 0.1802\n",
      "  Batch 40/391 - Loss: 0.1409\n",
      "  Batch 50/391 - Loss: 0.1250\n",
      "  Batch 60/391 - Loss: 0.1041\n",
      "  Batch 70/391 - Loss: 0.2067\n",
      "  Batch 80/391 - Loss: 0.1091\n",
      "  Batch 90/391 - Loss: 0.0746\n",
      "  Batch 100/391 - Loss: 0.1444\n",
      "  Batch 110/391 - Loss: 0.1007\n",
      "  Batch 120/391 - Loss: 0.1499\n",
      "  Batch 130/391 - Loss: 0.0927\n",
      "  Batch 140/391 - Loss: 0.1081\n",
      "  Batch 150/391 - Loss: 0.1267\n",
      "  Batch 160/391 - Loss: 0.1113\n",
      "  Batch 170/391 - Loss: 0.1528\n",
      "  Batch 180/391 - Loss: 0.1072\n",
      "  Batch 190/391 - Loss: 0.1542\n",
      "  Batch 200/391 - Loss: 0.1148\n",
      "  Batch 210/391 - Loss: 0.1443\n",
      "  Batch 220/391 - Loss: 0.1318\n",
      "  Batch 230/391 - Loss: 0.1620\n",
      "  Batch 240/391 - Loss: 0.1546\n",
      "  Batch 250/391 - Loss: 0.1471\n",
      "  Batch 260/391 - Loss: 0.0834\n",
      "  Batch 270/391 - Loss: 0.0951\n",
      "  Batch 280/391 - Loss: 0.0830\n",
      "  Batch 290/391 - Loss: 0.1164\n",
      "  Batch 300/391 - Loss: 0.1380\n",
      "  Batch 310/391 - Loss: 0.0910\n",
      "  Batch 320/391 - Loss: 0.0858\n",
      "  Batch 330/391 - Loss: 0.0969\n",
      "  Batch 340/391 - Loss: 0.1296\n",
      "  Batch 350/391 - Loss: 0.1014\n",
      "  Batch 360/391 - Loss: 0.1227\n",
      "  Batch 370/391 - Loss: 0.1035\n",
      "  Batch 380/391 - Loss: 0.1118\n",
      "  Batch 390/391 - Loss: 0.1090\n",
      "  Batch 391/391 - Loss: 0.1351\n",
      "Epoch 2 completed. Average Loss: 0.1244\n",
      "\n",
      "Epoch 3/10\n",
      "  Batch 10/391 - Loss: 0.1004\n",
      "  Batch 20/391 - Loss: 0.1238\n",
      "  Batch 30/391 - Loss: 0.0732\n",
      "  Batch 40/391 - Loss: 0.0920\n",
      "  Batch 50/391 - Loss: 0.0899\n",
      "  Batch 60/391 - Loss: 0.1351\n",
      "  Batch 70/391 - Loss: 0.0772\n",
      "  Batch 80/391 - Loss: 0.1543\n",
      "  Batch 90/391 - Loss: 0.0938\n",
      "  Batch 100/391 - Loss: 0.1406\n",
      "  Batch 110/391 - Loss: 0.0762\n",
      "  Batch 120/391 - Loss: 0.1038\n",
      "  Batch 130/391 - Loss: 0.1538\n",
      "  Batch 140/391 - Loss: 0.1409\n",
      "  Batch 150/391 - Loss: 0.1221\n",
      "  Batch 160/391 - Loss: 0.1285\n",
      "  Batch 170/391 - Loss: 0.1336\n",
      "  Batch 180/391 - Loss: 0.1355\n",
      "  Batch 190/391 - Loss: 0.1381\n",
      "  Batch 200/391 - Loss: 0.0958\n",
      "  Batch 210/391 - Loss: 0.0997\n",
      "  Batch 220/391 - Loss: 0.1180\n",
      "  Batch 230/391 - Loss: 0.1252\n",
      "  Batch 240/391 - Loss: 0.1265\n",
      "  Batch 250/391 - Loss: 0.1298\n",
      "  Batch 260/391 - Loss: 0.1252\n",
      "  Batch 270/391 - Loss: 0.1042\n",
      "  Batch 280/391 - Loss: 0.1002\n",
      "  Batch 290/391 - Loss: 0.1049\n",
      "  Batch 300/391 - Loss: 0.0740\n",
      "  Batch 310/391 - Loss: 0.1041\n",
      "  Batch 320/391 - Loss: 0.0860\n",
      "  Batch 330/391 - Loss: 0.1409\n",
      "  Batch 340/391 - Loss: 0.1128\n",
      "  Batch 350/391 - Loss: 0.1151\n",
      "  Batch 360/391 - Loss: 0.0948\n",
      "  Batch 370/391 - Loss: 0.1200\n",
      "  Batch 380/391 - Loss: 0.1055\n",
      "  Batch 390/391 - Loss: 0.1268\n",
      "  Batch 391/391 - Loss: 0.0829\n",
      "Epoch 3 completed. Average Loss: 0.1127\n",
      "\n",
      "Epoch 4/10\n",
      "  Batch 10/391 - Loss: 0.0536\n",
      "  Batch 20/391 - Loss: 0.1101\n",
      "  Batch 30/391 - Loss: 0.0963\n",
      "  Batch 40/391 - Loss: 0.1023\n",
      "  Batch 50/391 - Loss: 0.1523\n",
      "  Batch 60/391 - Loss: 0.0945\n",
      "  Batch 70/391 - Loss: 0.1133\n",
      "  Batch 80/391 - Loss: 0.0994\n",
      "  Batch 90/391 - Loss: 0.0787\n",
      "  Batch 100/391 - Loss: 0.0815\n",
      "  Batch 110/391 - Loss: 0.0957\n",
      "  Batch 120/391 - Loss: 0.0832\n",
      "  Batch 130/391 - Loss: 0.0684\n",
      "  Batch 140/391 - Loss: 0.1315\n",
      "  Batch 150/391 - Loss: 0.0866\n",
      "  Batch 160/391 - Loss: 0.0872\n",
      "  Batch 170/391 - Loss: 0.1530\n",
      "  Batch 180/391 - Loss: 0.1079\n",
      "  Batch 190/391 - Loss: 0.1047\n",
      "  Batch 200/391 - Loss: 0.1426\n",
      "  Batch 210/391 - Loss: 0.1081\n",
      "  Batch 220/391 - Loss: 0.1216\n",
      "  Batch 230/391 - Loss: 0.1242\n",
      "  Batch 240/391 - Loss: 0.1366\n",
      "  Batch 250/391 - Loss: 0.1288\n",
      "  Batch 260/391 - Loss: 0.1199\n",
      "  Batch 270/391 - Loss: 0.0805\n",
      "  Batch 280/391 - Loss: 0.0803\n",
      "  Batch 290/391 - Loss: 0.1475\n",
      "  Batch 300/391 - Loss: 0.0630\n",
      "  Batch 310/391 - Loss: 0.1312\n",
      "  Batch 320/391 - Loss: 0.1047\n",
      "  Batch 330/391 - Loss: 0.0752\n",
      "  Batch 340/391 - Loss: 0.0972\n",
      "  Batch 350/391 - Loss: 0.0989\n",
      "  Batch 360/391 - Loss: 0.0679\n",
      "  Batch 370/391 - Loss: 0.0979\n",
      "  Batch 380/391 - Loss: 0.0896\n",
      "  Batch 390/391 - Loss: 0.0811\n",
      "  Batch 391/391 - Loss: 0.0684\n",
      "Epoch 4 completed. Average Loss: 0.1007\n",
      "\n",
      "Epoch 5/10\n",
      "  Batch 10/391 - Loss: 0.0626\n",
      "  Batch 20/391 - Loss: 0.1055\n",
      "  Batch 30/391 - Loss: 0.0785\n",
      "  Batch 40/391 - Loss: 0.0722\n",
      "  Batch 50/391 - Loss: 0.0891\n",
      "  Batch 60/391 - Loss: 0.0674\n",
      "  Batch 70/391 - Loss: 0.0851\n",
      "  Batch 80/391 - Loss: 0.0691\n",
      "  Batch 90/391 - Loss: 0.0583\n",
      "  Batch 100/391 - Loss: 0.0863\n",
      "  Batch 110/391 - Loss: 0.1167\n",
      "  Batch 120/391 - Loss: 0.0818\n",
      "  Batch 130/391 - Loss: 0.0970\n",
      "  Batch 140/391 - Loss: 0.1247\n",
      "  Batch 150/391 - Loss: 0.0607\n",
      "  Batch 160/391 - Loss: 0.0987\n",
      "  Batch 170/391 - Loss: 0.0691\n",
      "  Batch 180/391 - Loss: 0.1002\n",
      "  Batch 190/391 - Loss: 0.0746\n",
      "  Batch 200/391 - Loss: 0.0722\n",
      "  Batch 210/391 - Loss: 0.1196\n",
      "  Batch 220/391 - Loss: 0.0927\n",
      "  Batch 230/391 - Loss: 0.0919\n",
      "  Batch 240/391 - Loss: 0.0921\n",
      "  Batch 250/391 - Loss: 0.0657\n",
      "  Batch 260/391 - Loss: 0.1170\n",
      "  Batch 270/391 - Loss: 0.0844\n",
      "  Batch 280/391 - Loss: 0.0742\n",
      "  Batch 290/391 - Loss: 0.0865\n",
      "  Batch 300/391 - Loss: 0.0986\n",
      "  Batch 310/391 - Loss: 0.0806\n",
      "  Batch 320/391 - Loss: 0.0801\n",
      "  Batch 330/391 - Loss: 0.0918\n",
      "  Batch 340/391 - Loss: 0.0755\n",
      "  Batch 350/391 - Loss: 0.0671\n",
      "  Batch 360/391 - Loss: 0.0807\n",
      "  Batch 370/391 - Loss: 0.1236\n",
      "  Batch 380/391 - Loss: 0.0970\n",
      "  Batch 390/391 - Loss: 0.0563\n",
      "  Batch 391/391 - Loss: 0.0958\n",
      "Epoch 5 completed. Average Loss: 0.0898\n",
      "\n",
      "Epoch 6/10\n",
      "  Batch 10/391 - Loss: 0.0638\n",
      "  Batch 20/391 - Loss: 0.0581\n",
      "  Batch 30/391 - Loss: 0.1017\n",
      "  Batch 40/391 - Loss: 0.0331\n",
      "  Batch 50/391 - Loss: 0.0655\n",
      "  Batch 60/391 - Loss: 0.0746\n",
      "  Batch 70/391 - Loss: 0.0746\n",
      "  Batch 80/391 - Loss: 0.0837\n",
      "  Batch 90/391 - Loss: 0.0623\n",
      "  Batch 100/391 - Loss: 0.0473\n",
      "  Batch 110/391 - Loss: 0.0987\n",
      "  Batch 120/391 - Loss: 0.0692\n",
      "  Batch 130/391 - Loss: 0.0899\n",
      "  Batch 140/391 - Loss: 0.0855\n",
      "  Batch 150/391 - Loss: 0.0916\n",
      "  Batch 160/391 - Loss: 0.0826\n",
      "  Batch 170/391 - Loss: 0.0963\n",
      "  Batch 180/391 - Loss: 0.1161\n",
      "  Batch 190/391 - Loss: 0.0680\n",
      "  Batch 200/391 - Loss: 0.0738\n",
      "  Batch 210/391 - Loss: 0.0470\n",
      "  Batch 220/391 - Loss: 0.1111\n",
      "  Batch 230/391 - Loss: 0.1094\n",
      "  Batch 240/391 - Loss: 0.0607\n",
      "  Batch 250/391 - Loss: 0.0704\n",
      "  Batch 260/391 - Loss: 0.0969\n",
      "  Batch 270/391 - Loss: 0.0608\n",
      "  Batch 280/391 - Loss: 0.1256\n",
      "  Batch 290/391 - Loss: 0.0935\n",
      "  Batch 300/391 - Loss: 0.0667\n",
      "  Batch 310/391 - Loss: 0.0568\n",
      "  Batch 320/391 - Loss: 0.0718\n",
      "  Batch 330/391 - Loss: 0.0704\n",
      "  Batch 340/391 - Loss: 0.0876\n",
      "  Batch 350/391 - Loss: 0.0886\n",
      "  Batch 360/391 - Loss: 0.0935\n",
      "  Batch 370/391 - Loss: 0.1008\n",
      "  Batch 380/391 - Loss: 0.0531\n",
      "  Batch 390/391 - Loss: 0.0758\n",
      "  Batch 391/391 - Loss: 0.0600\n",
      "Epoch 6 completed. Average Loss: 0.0769\n",
      "\n",
      "Epoch 7/10\n",
      "  Batch 10/391 - Loss: 0.0548\n",
      "  Batch 20/391 - Loss: 0.0454\n",
      "  Batch 30/391 - Loss: 0.0420\n",
      "  Batch 40/391 - Loss: 0.0320\n",
      "  Batch 50/391 - Loss: 0.0384\n",
      "  Batch 60/391 - Loss: 0.0584\n",
      "  Batch 70/391 - Loss: 0.0742\n",
      "  Batch 80/391 - Loss: 0.0597\n",
      "  Batch 90/391 - Loss: 0.0365\n",
      "  Batch 100/391 - Loss: 0.0404\n",
      "  Batch 110/391 - Loss: 0.0715\n",
      "  Batch 120/391 - Loss: 0.0399\n",
      "  Batch 130/391 - Loss: 0.0426\n",
      "  Batch 140/391 - Loss: 0.0636\n",
      "  Batch 150/391 - Loss: 0.1130\n",
      "  Batch 160/391 - Loss: 0.0900\n",
      "  Batch 170/391 - Loss: 0.0867\n",
      "  Batch 180/391 - Loss: 0.0706\n",
      "  Batch 190/391 - Loss: 0.0295\n",
      "  Batch 200/391 - Loss: 0.0561\n",
      "  Batch 210/391 - Loss: 0.0864\n",
      "  Batch 220/391 - Loss: 0.0462\n",
      "  Batch 230/391 - Loss: 0.0461\n",
      "  Batch 240/391 - Loss: 0.0500\n",
      "  Batch 250/391 - Loss: 0.0729\n",
      "  Batch 260/391 - Loss: 0.0341\n",
      "  Batch 270/391 - Loss: 0.0740\n",
      "  Batch 280/391 - Loss: 0.0672\n",
      "  Batch 290/391 - Loss: 0.0574\n",
      "  Batch 300/391 - Loss: 0.0524\n",
      "  Batch 310/391 - Loss: 0.1171\n",
      "  Batch 320/391 - Loss: 0.0622\n",
      "  Batch 330/391 - Loss: 0.0693\n",
      "  Batch 340/391 - Loss: 0.0586\n",
      "  Batch 350/391 - Loss: 0.0602\n",
      "  Batch 360/391 - Loss: 0.0813\n",
      "  Batch 370/391 - Loss: 0.0593\n",
      "  Batch 380/391 - Loss: 0.0648\n",
      "  Batch 390/391 - Loss: 0.0798\n",
      "  Batch 391/391 - Loss: 0.0766\n",
      "Epoch 7 completed. Average Loss: 0.0640\n",
      "\n",
      "Epoch 8/10\n",
      "  Batch 10/391 - Loss: 0.0647\n",
      "  Batch 20/391 - Loss: 0.0485\n",
      "  Batch 30/391 - Loss: 0.0436\n",
      "  Batch 40/391 - Loss: 0.0477\n",
      "  Batch 50/391 - Loss: 0.0189\n",
      "  Batch 60/391 - Loss: 0.0317\n",
      "  Batch 70/391 - Loss: 0.0575\n",
      "  Batch 80/391 - Loss: 0.0468\n",
      "  Batch 90/391 - Loss: 0.0613\n",
      "  Batch 100/391 - Loss: 0.0363\n",
      "  Batch 110/391 - Loss: 0.0176\n",
      "  Batch 120/391 - Loss: 0.0366\n",
      "  Batch 130/391 - Loss: 0.0429\n",
      "  Batch 140/391 - Loss: 0.0528\n",
      "  Batch 150/391 - Loss: 0.0533\n",
      "  Batch 160/391 - Loss: 0.0616\n",
      "  Batch 170/391 - Loss: 0.0997\n",
      "  Batch 180/391 - Loss: 0.0165\n",
      "  Batch 190/391 - Loss: 0.0322\n",
      "  Batch 200/391 - Loss: 0.0679\n",
      "  Batch 210/391 - Loss: 0.0450\n",
      "  Batch 220/391 - Loss: 0.0746\n",
      "  Batch 230/391 - Loss: 0.0724\n",
      "  Batch 240/391 - Loss: 0.0428\n",
      "  Batch 250/391 - Loss: 0.0457\n",
      "  Batch 260/391 - Loss: 0.0636\n",
      "  Batch 270/391 - Loss: 0.0376\n",
      "  Batch 280/391 - Loss: 0.0492\n",
      "  Batch 290/391 - Loss: 0.0520\n",
      "  Batch 300/391 - Loss: 0.1132\n",
      "  Batch 310/391 - Loss: 0.0318\n",
      "  Batch 320/391 - Loss: 0.0744\n",
      "  Batch 330/391 - Loss: 0.0415\n",
      "  Batch 340/391 - Loss: 0.0880\n",
      "  Batch 350/391 - Loss: 0.0567\n",
      "  Batch 360/391 - Loss: 0.0766\n",
      "  Batch 370/391 - Loss: 0.0395\n",
      "  Batch 380/391 - Loss: 0.0549\n",
      "  Batch 390/391 - Loss: 0.0608\n",
      "  Batch 391/391 - Loss: 0.0525\n",
      "Epoch 8 completed. Average Loss: 0.0501\n",
      "\n",
      "Epoch 9/10\n",
      "  Batch 10/391 - Loss: 0.0372\n",
      "  Batch 20/391 - Loss: 0.0370\n",
      "  Batch 30/391 - Loss: 0.0224\n",
      "  Batch 40/391 - Loss: 0.0208\n",
      "  Batch 50/391 - Loss: 0.0319\n",
      "  Batch 60/391 - Loss: 0.0568\n",
      "  Batch 70/391 - Loss: 0.0354\n",
      "  Batch 80/391 - Loss: 0.0542\n",
      "  Batch 90/391 - Loss: 0.0456\n",
      "  Batch 100/391 - Loss: 0.0412\n",
      "  Batch 110/391 - Loss: 0.0413\n",
      "  Batch 120/391 - Loss: 0.0417\n",
      "  Batch 130/391 - Loss: 0.0194\n",
      "  Batch 140/391 - Loss: 0.0521\n",
      "  Batch 150/391 - Loss: 0.0311\n",
      "  Batch 160/391 - Loss: 0.0558\n",
      "  Batch 170/391 - Loss: 0.0336\n",
      "  Batch 180/391 - Loss: 0.0167\n",
      "  Batch 190/391 - Loss: 0.0353\n",
      "  Batch 200/391 - Loss: 0.0505\n",
      "  Batch 210/391 - Loss: 0.0416\n",
      "  Batch 220/391 - Loss: 0.0676\n",
      "  Batch 230/391 - Loss: 0.0422\n",
      "  Batch 240/391 - Loss: 0.0368\n",
      "  Batch 250/391 - Loss: 0.0456\n",
      "  Batch 260/391 - Loss: 0.0386\n",
      "  Batch 270/391 - Loss: 0.0366\n",
      "  Batch 280/391 - Loss: 0.0578\n",
      "  Batch 290/391 - Loss: 0.0320\n",
      "  Batch 300/391 - Loss: 0.0503\n",
      "  Batch 310/391 - Loss: 0.1244\n",
      "  Batch 320/391 - Loss: 0.0309\n",
      "  Batch 330/391 - Loss: 0.0854\n",
      "  Batch 340/391 - Loss: 0.0359\n",
      "  Batch 350/391 - Loss: 0.0837\n",
      "  Batch 360/391 - Loss: 0.0311\n",
      "  Batch 370/391 - Loss: 0.0228\n",
      "  Batch 380/391 - Loss: 0.0417\n",
      "  Batch 390/391 - Loss: 0.0429\n",
      "  Batch 391/391 - Loss: 0.0521\n",
      "Epoch 9 completed. Average Loss: 0.0400\n",
      "\n",
      "Epoch 10/10\n",
      "  Batch 10/391 - Loss: 0.0148\n",
      "  Batch 20/391 - Loss: 0.0375\n",
      "  Batch 30/391 - Loss: 0.0131\n",
      "  Batch 40/391 - Loss: 0.0156\n",
      "  Batch 50/391 - Loss: 0.0267\n",
      "  Batch 60/391 - Loss: 0.0236\n",
      "  Batch 70/391 - Loss: 0.0263\n",
      "  Batch 80/391 - Loss: 0.0477\n",
      "  Batch 90/391 - Loss: 0.0239\n",
      "  Batch 100/391 - Loss: 0.0274\n",
      "  Batch 110/391 - Loss: 0.0390\n",
      "  Batch 120/391 - Loss: 0.0211\n",
      "  Batch 130/391 - Loss: 0.0334\n",
      "  Batch 140/391 - Loss: 0.0174\n",
      "  Batch 150/391 - Loss: 0.0637\n",
      "  Batch 160/391 - Loss: 0.0227\n",
      "  Batch 170/391 - Loss: 0.0551\n",
      "  Batch 180/391 - Loss: 0.0200\n",
      "  Batch 190/391 - Loss: 0.0390\n",
      "  Batch 200/391 - Loss: 0.0155\n",
      "  Batch 210/391 - Loss: 0.0344\n",
      "  Batch 220/391 - Loss: 0.0358\n",
      "  Batch 230/391 - Loss: 0.0915\n",
      "  Batch 240/391 - Loss: 0.0400\n",
      "  Batch 250/391 - Loss: 0.0349\n",
      "  Batch 260/391 - Loss: 0.0252\n",
      "  Batch 270/391 - Loss: 0.0084\n",
      "  Batch 280/391 - Loss: 0.0345\n",
      "  Batch 290/391 - Loss: 0.0197\n",
      "  Batch 300/391 - Loss: 0.0154\n",
      "  Batch 310/391 - Loss: 0.0479\n",
      "  Batch 320/391 - Loss: 0.0154\n",
      "  Batch 330/391 - Loss: 0.0607\n",
      "  Batch 340/391 - Loss: 0.0369\n",
      "  Batch 350/391 - Loss: 0.0511\n",
      "  Batch 360/391 - Loss: 0.0350\n",
      "  Batch 370/391 - Loss: 0.0341\n",
      "  Batch 380/391 - Loss: 0.0280\n",
      "  Batch 390/391 - Loss: 0.0164\n",
      "  Batch 391/391 - Loss: 0.0336\n",
      "Epoch 10 completed. Average Loss: 0.0296\n",
      " Model saved to './mymodel.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8992/454487649.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./mymodel.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "  Processed batch 10/79\n",
      "  Processed batch 20/79\n",
      "  Processed batch 30/79\n",
      "  Processed batch 40/79\n",
      "  Processed batch 50/79\n",
      "  Processed batch 60/79\n",
      "  Processed batch 70/79\n",
      "  Processed batch 79/79\n",
      " Predictions saved to './label_val.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8992/454487649.py:86: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f.write(f\"{int(pred > 0.5)}\\n\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    train_img_dir = './ml_exercise_therapanacea/train_img/'\n",
    "    train_labels_file = './ml_exercise_therapanacea/label_train.txt'\n",
    "    val_img_dir = './ml_exercise_therapanacea/val_img/'\n",
    "\n",
    "    train_loader, val_loader = get_data_loaders(train_img_dir, train_labels_file, val_img_dir)\n",
    "\n",
    "    train_model(train_loader)\n",
    "    predict(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484949cf-854a-4d2f-9d15-040f827aa44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfbd5f-4262-42c1-b805-64bc6e16e96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7478239,
     "sourceId": 11896788,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
